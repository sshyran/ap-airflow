version: 2.1
workflows:
  version: 2.1

  regression:
    jobs:
      - waiter
{%- for ac_version, distributions in image_map.items() %}
{%- set airflow_version = ac_version | get_airflow_version -%}
{%- set dev_build = "true" if "dev" in ac_version else "false" -%}
{%- for distribution in distributions %}
    {%- if airflow_version|checkair2 == true %}
      - qa-ui:
          name: qa-ui-regression-celery-{{ airflow_version }}-{{ distribution }}
          airflow_version: {{ airflow_version }}
          dev_build: {{ dev_build }}
          extra_tags: "{{ airflow_version }}-{{ distribution }}-onbuild-${CIRCLE_BUILD_NUM}"
          executor_type: "celery"
          test_type: "testim"
          requires:
            - waiter
      - qa-ui:
          name: qa-ui-regression-k8s-{{ airflow_version }}-{{ distribution }}
          airflow_version: {{ airflow_version }}
          dev_build: {{ dev_build }}
          extra_tags: "{{ airflow_version }}-{{ distribution }}-onbuild-${CIRCLE_BUILD_NUM}"
          executor_type: "kubernetes"
          test_type: "testim"
          requires:
            - qa-ui-regression-celery-{{ airflow_version }}-{{ distribution }}
      - qa-ui:
          name: qa-ui-regression-local-{{ airflow_version }}-{{ distribution }}
          airflow_version: {{ airflow_version }}
          dev_build: {{ dev_build }}
          extra_tags: "{{ airflow_version }}-{{ distribution }}-onbuild-${CIRCLE_BUILD_NUM}"
          executor_type: "local"
          test_type: "testim"
          requires:
            - qa-ui-regression-k8s-{{ airflow_version }}-{{ distribution }}
      - qa-core:
          name: qa-core-celery-{{ airflow_version }}-{{ distribution }}
          airflow_version: {{ airflow_version }}
          dev_build: {{ dev_build }}
          extra_tags: "{{ airflow_version }}-{{ distribution }}-onbuild-${CIRCLE_BUILD_NUM}"
          executor_type: "celery"
          test_type: "core"
          requires:
            - waiter
      # - qa-negative:
      #     name: qa-negative-celery-{{ airflow_version }}-{{ distribution }}
      #     airflow_version: {{ airflow_version }}
      #     dev_build: {{ dev_build }}
      #     extra_tags: "{{ airflow_version }}-{{ distribution }}-onbuild-${CIRCLE_BUILD_NUM}"
      #     executor_type: "celery"
      #     test_type: "core"
      #     requires:
      #       - qa-core
      - qa-k8s:
          name: qa-k8s-celery-{{ airflow_version }}-{{ distribution }}
          airflow_version: {{ airflow_version }}
          dev_build: {{ dev_build }}
          extra_tags: "{{ airflow_version }}-{{ distribution }}-onbuild-${CIRCLE_BUILD_NUM}"
          executor_type: "celery"
          test_type: "k8s"
          requires:
            - qa-core-celery-{{ airflow_version }}-{{ distribution }}
      - qa-params:
          name: qa-params-celery-{{ airflow_version }}-{{ distribution }}
          airflow_version: {{ airflow_version }}
          dev_build: {{ dev_build }}
          extra_tags: "{{ airflow_version }}-{{ distribution }}-onbuild-${CIRCLE_BUILD_NUM}"
          executor_type: "celery"
          test_type: "params"
          requires:
            - qa-k8s-celery-{{ airflow_version }}-{{ distribution }}
      - qa-restapi:
          name: qa-restapi-celery-{{ airflow_version }}-{{ distribution }}
          airflow_version: {{ airflow_version }}
          dev_build: {{ dev_build }}
          extra_tags: "{{ airflow_version }}-{{ distribution }}-onbuild-${CIRCLE_BUILD_NUM}"
          executor_type: "celery"
          test_type: "rest"
          requires:
            - waiter
      - qa-restapi:
          name: qa-restapi-local-{{ airflow_version }}-{{ distribution }}
          airflow_version: {{ airflow_version }}
          dev_build: {{ dev_build }}
          extra_tags: "{{ airflow_version }}-{{ distribution }}-onbuild-${CIRCLE_BUILD_NUM}"
          executor_type: "local"
          test_type: "rest"
          requires:
            - qa-restapi-celery-{{ airflow_version }}-{{ distribution }}
      - qa-restapi:
          name: qa-restapi-k8s-{{ airflow_version }}-{{ distribution }}
          airflow_version: {{ airflow_version }}
          dev_build: {{ dev_build }}
          extra_tags: "{{ airflow_version }}-{{ distribution }}-onbuild-${CIRCLE_BUILD_NUM}"
          executor_type: "kubernetes"
          test_type: "rest"
          requires:
            - qa-restapi-local-{{ airflow_version }}-{{ distribution }}
      - qa-core:
          name: qa-core-k8s-{{ airflow_version }}-{{ distribution }}
          airflow_version: {{ airflow_version }}
          dev_build: {{ dev_build }}
          extra_tags: "{{ airflow_version }}-{{ distribution }}-onbuild-${CIRCLE_BUILD_NUM}"
          executor_type: "kubernetes"
          test_type: "core"
          requires:
            - waiter
      - qa-k8s:
          name: qa-k8s-k8s-{{ airflow_version }}-{{ distribution }}
          airflow_version: {{ airflow_version }}
          dev_build: {{ dev_build }}
          extra_tags: "{{ airflow_version }}-{{ distribution }}-onbuild-${CIRCLE_BUILD_NUM}"
          executor_type: "kubernetes"
          test_type: "k8s"
          requires:
            - qa-core-k8s-{{ airflow_version }}-{{ distribution }}
      - qa-params:
          name: qa-params-k8s-{{ airflow_version }}-{{ distribution }}
          airflow_version: {{ airflow_version }}
          dev_build: {{ dev_build }}
          extra_tags: "{{ airflow_version }}-{{ distribution }}-onbuild-${CIRCLE_BUILD_NUM}"
          executor_type: "kubernetes"
          test_type: "params"
          requires:
            - qa-k8s-k8s-{{ airflow_version }}-{{ distribution }}
      - qa-negative:
          name: qa-negative-k8s-{{ airflow_version }}-{{ distribution }}
          airflow_version: {{ airflow_version }}
          dev_build: {{ dev_build }}
          extra_tags: "{{ airflow_version }}-{{ distribution }}-onbuild-${CIRCLE_BUILD_NUM}"
          executor_type: "kubernetes"
          test_type: "negative"
          requires:
             - qa-params-k8s-{{ airflow_version }}-{{ distribution }}
      - qa-core:
          name: qa-core-local-{{ airflow_version }}-{{ distribution }}
          airflow_version: {{ airflow_version }}
          dev_build: {{ dev_build }}
          extra_tags: "{{ airflow_version }}-{{ distribution }}-onbuild-${CIRCLE_BUILD_NUM}"
          executor_type: "local"
          test_type: "core"
          requires:
            - waiter
    {%- endif %}
{%- endfor %}
{%- endfor %}
  certified-airflow:
    jobs:
      - static-checks
{%- for ac_version, distributions in image_map.items() %}
{%- set airflow_version = ac_version | get_airflow_version -%}
{%- set dev_build = "true" if "dev" in ac_version else "false" -%}

{% for distribution in distributions %}
      - build:
          name: build-{{ airflow_version }}-{{ distribution }}
          airflow_version: {{ airflow_version }}
          distribution_name: {{ distribution }}
          dev_build: {{ dev_build }}
          {%- if "dev" in ac_version and airflow_version not in dev_allowlist %}
          extra_args: "--build-arg VERSION=$(curl https://pip.astronomer.io/simple/astronomer-certified/latest-{{ airflow_version | replace('.dev', '') }}.build)"
          {%- endif %}
          requires:
            - static-checks
      # - scan-clair:
      #     name: scan-clair-{{ airflow_version }}-{{ distribution }}-onbuild
      #     airflow_version: {{ airflow_version }}
      #     distribution_name: {{ distribution }}-onbuild
      #     requires:
      #       - build-{{ airflow_version }}-{{ distribution }}
      # - scan-trivy:
      #     name: scan-trivy-{{ airflow_version }}-{{ distribution }}-onbuild
      #     airflow_version: {{ airflow_version }}
      #     distribution: {{ distribution }}
      #     distribution_name: {{ distribution }}-onbuild
      #     requires:
      #       - build-{{ airflow_version }}-{{ distribution }}
      - test:
          name: test-{{ airflow_version }}-{{ distribution }}-images
          tag: "{{ airflow_version }}-{{ distribution }}"
          requires:
            - build-{{ airflow_version }}-{{ distribution }}
      - push:
          name: push-{{ airflow_version }}-{{ distribution }}
          tag: "{{ airflow_version }}-{{ distribution }}"
          dev_build: {{ dev_build }}
          extra_tags: "{{ airflow_version }}-{{ distribution }}-${CIRCLE_BUILD_NUM},{{ ac_version }}-{{ distribution }}"
          context:
            - quay.io
          requires:
            # - scan-clair-{{ airflow_version }}-{{ distribution }}-onbuild
            # - scan-trivy-{{ airflow_version }}-{{ distribution }}-onbuild
            - test-{{ airflow_version }}-{{ distribution }}-images
          filters:
            branches:
              only:
                - master
      - push:
          name: push-{{ airflow_version }}-{{ distribution }}-onbuild
          tag: "{{ airflow_version }}-{{ distribution }}-onbuild"
          dev_build: {{ dev_build }}
          extra_tags: "{{ airflow_version }}-{{ distribution }}-onbuild-${CIRCLE_BUILD_NUM},{{ ac_version }}-{{ distribution }}-onbuild"
          context:
            - quay.io
          requires:
            # - scan-clair-{{ airflow_version }}-{{ distribution }}-onbuild
            # - scan-trivy-{{ airflow_version }}-{{ distribution }}-onbuild
            - test-{{ airflow_version }}-{{ distribution }}-images
          filters:
            branches:
              only:
                - master
                - testim_workflows
                - waiter_for_each_airflowv
{%- endfor %}
{%- endfor %}
{%- if image_map.keys() | dev_releases | length > 0 %}
  nightly:
    triggers:
      - schedule:
          cron: "0 0 * * *"
          filters:
            branches:
              only:
                - master
    jobs:
{%- for ac_version, distributions in image_map.items() %}
{%- set airflow_version = ac_version | get_airflow_version %}
{%- if "dev" in ac_version and airflow_version not in dev_allowlist  %}
{%- for distribution in distributions %}
      - build:
          name: build-{{ airflow_version }}-{{ distribution }}
          airflow_version: {{ airflow_version }}
          distribution_name: {{ distribution }}
          dev_build: true
          extra_args: "--build-arg VERSION=$(curl https://pip.astronomer.io/simple/astronomer-certified/latest-{{ airflow_version | replace('.dev', '') }}.build)"
      # - scan-clair:
      #     name: scan-clair-{{ airflow_version }}-{{ distribution }}-onbuild
      #     airflow_version: {{ airflow_version }}
      #     distribution_name: {{ distribution }}-onbuild
      #     requires:
      #       - build-{{ airflow_version }}-{{ distribution }}
      # - scan-trivy:
      #     name: scan-trivy-{{ airflow_version }}-{{ distribution }}-onbuild
      #     airflow_version: {{ airflow_version }}
      #     distribution: {{ distribution }}
      #     distribution_name: {{ distribution }}-onbuild
      #     requires:
      #       - build-{{ airflow_version }}-{{ distribution }}
      - test:
          name: test-{{ airflow_version }}-{{ distribution }}-images
          tag: "{{ airflow_version }}-{{ distribution }}"
          requires:
            - build-{{ airflow_version }}-{{ distribution }}
      - push:
          name: push-{{ airflow_version }}-{{ distribution }}
          tag: "{{ airflow_version }}-{{ distribution }}"
          dev_build: true
          extra_tags: "{{ airflow_version }}-{{ distribution }}-${CIRCLE_BUILD_NUM}"
          context:
            - quay.io
          requires:
            # - scan-clair-{{ airflow_version }}-{{ distribution }}-onbuild
            # - scan-trivy-{{ airflow_version }}-{{ distribution }}-onbuild
            - test-{{ airflow_version }}-{{ distribution }}-images
          filters:
            branches:
              only:
                - master
      - push:
          name: push-{{ airflow_version }}-{{ distribution }}-onbuild
          tag: "{{ airflow_version }}-{{ distribution }}-onbuild"
          dev_build: true
          extra_tags: "{{ airflow_version }}-{{ distribution }}-onbuild-${CIRCLE_BUILD_NUM},{{ ac_version }}-{{ distribution }}-onbuild"
          context:
            - quay.io
          requires:
            # - scan-clair-{{ airflow_version }}-{{ distribution }}-onbuild
            # - scan-trivy-{{ airflow_version }}-{{ distribution }}-onbuild
            - test-{{ airflow_version }}-{{ distribution }}-images
          filters:
            branches:
              only:
                - master
                - testim_workflows
{%- endfor %}
{%- endif %}
{%- endfor %}
{% endif %}
jobs:
  waiter:
    executor: docker-executor
    resource_class: small
    steps:
      #- run: sleep 120
      - wait_workflow_orb:
          workflow-name: certified-airflow
          max-wait-time: '100000'
          sleep-time-between-checks: '60'
          branch-to-consider: ${CIRCLE_BRANCH}
  qa-ui:
    description: "Run UI Regression Tests using Testim"
    environment:
      CIRCLE_TEST_REPORTS: /tmp/test-results
    machine: # executor type
      image: ubuntu-2004:202010-01
    parameters:
      airflow_version:
        description: "The Airflow version, for example '1.10.5'"
        type: string
      dev_build:
        description: "Indicate if this is a dev build"
        type: boolean
      extra_tags:
        description: "Extra args to pass to pass to Docker build command"
        type: string
      executor_type:
        description: "The type of executor to run on"
        default: "celery"
        type: string
      test_type:
        description: "The type of framework to run"
        default: ""
        type: string
    steps:
      - run: sudo apt-get -y update
      - run: sudo apt-get -y install git
      - run: sudo apt-get -y install curl
      - run: sudo apt-get -y install wget
      - run: curl -sL https://deb.nodesource.com/setup_12.x | sudo -E bash -
      - run: sudo apt-get install nodejs
      - run: npm -v
      - run: node -v
      - run:
          name: Avoid hosts unknown for github
          command:   |
            [ ! -d ~/.ssh ] && mkdir ~/.ssh/
            echo -e "Host github.com\n\tStrictHostKeyChecking no\n" > ~/.ssh/config
      - run: git clone git@github.com:astronomer/qa-regression.git
      - run: git clone git@github.com:astronomer/qa-scenario-dags.git
      - run:
          name: Run Shell Script for creating deployments
          command:  |
            cd qa-regression
            ls
            chmod 744 *.sh
            echo "Run Deployment Script"
            bash ./create_deployment.sh << parameters.airflow_version >>  << parameters.executor_type >>  << parameters.test_type >>
            cd ..
      - run: mkdir -p $CIRCLE_TEST_REPORTS/testim/
      - run:
          name: Run the Testim Tests
          command:   |
            npm i -g @testim/testim-cli
            cd /home/circleci/project/qa-regression
            testim --report-file $CIRCLE_TEST_REPORTS/testim/results.xml -c airflowui_testim_config.js --parallel 20 --retries 3 --result-label << parameters.extra_tags >> --test-config "1280x1024_SXGA_chrome" --test-config "1366x768_WXGA_firefox" --mode selenium
      - store_artifacts:
          path: /tmp/test-results
      - store_test_results:
          path: /tmp/test-results
  qa-core:
    description: "Run the Core DAGS"
    environment:
      CORE_TEST_REPORTS: /tmp/test-results
    machine: # executor type
      image: ubuntu-2004:202010-01
    parameters:
      airflow_version:
        description: "The Airflow version, for example '1.10.5'"
        type: string
      dev_build:
        description: "Indicate if this is a dev build"
        type: boolean
      extra_tags:
        description: "Extra args to pass to pass to Docker build command"
        type: string
      executor_type:
        description: "The type of executor to run on"
        default: "celery"
        type: string
      test_type:
        description: "The type of framework to run"
        default: ""
        type: string
    steps:
      - run: sudo apt-get -y update
      - run: sudo apt-get -y install git curl wget libpq-dev nodejs
      - run: curl -sL https://deb.nodesource.com/setup_12.x | sudo -E bash -
      - run: npm -v
      - run: node -v
      - run:
          name: Avoid hosts unknown for github
          command:   |
            [ ! -d ~/.ssh ] && mkdir ~/.ssh/
            echo -e "Host github.com\n\tStrictHostKeyChecking no\n" > ~/.ssh/config
      - run: git clone git@github.com:astronomer/qa-regression.git
      - run: git clone git@github.com:astronomer/qa-scenario-dags.git
      - run:
          name: Run Shell Script for creating deployments
          command:  |
            cd qa-regression
            ls
            chmod 744 *.sh
            echo "Run Deployment Script"
            bash ./create_deployment.sh << parameters.airflow_version >>  << parameters.executor_type >>  << parameters.test_type >>
            cd ..
      - run: mkdir -p $CORE_TEST_REPORTS/core/
      - run:
          name: Run the DAGS
          command:   |
            cd /home/circleci/project/qa-scenario-dags/scripts/dagstest
            pip3 install -e .
            python3 -m venv env
            . env/bin/activate
            fileurl="/home/circleci/project/env_current_api_url"
            export AIRFLOW_API_URL=$(cat "$fileurl")
            export DAGSTEST_DB_HOST=suleiman.db.elephantsql.com
            export DAGSTEST_DB_PORT=5432
            export DAGSTEST_DB_NAME=axsrggsp
            export DAGSTEST_DB_USER=axsrggsp
            export DAGSTEST_DB_PASSWORD=$QA_DAGSTEST_DB_PASSWORD
            dags connections --headers "Authorization":"Bearer $QA_CLUSTER_SERVICEACCOUNT" -f /home/circleci/project/qa-scenario-dags/airflow_settings.yaml
            EXEC_TIME=$(date +%FT%TZ)
            dags trigger --headers "Authorization":"$QA_CLUSTER_SERVICEACCOUNT" -e $EXEC_TIME -t core
            sleep 240
            echo "Sleeping for 240s"
            dags results -e $EXEC_TIME --headers "Authorization":"Bearer $QA_CLUSTER_SERVICEACCOUNT" > /tmp/test-results/results_core_dags.out
            # FOUND_FAILURES=0
            # grep -q "failed" /tmp/test-results/results_core_dags.out && FOUND_FAILURES=1
            # cat /tmp/test-results/results_core_dags.out
            # if [[ "$FOUND_FAILURES" -eq 1 ]]; then echo "DAG Failures Found";exit 1; fi
            # cd /home/circleci/project/qa-regression
            # chmod 744 delete_deployment.sh
            # bash ./delete_deployment.sh
      - store_artifacts:
          path: /tmp/test-results
      - store_test_results:
          path: /tmp/test-results
      - persist_to_workspace:
          root: ~/
          paths:
            - project
  qa-restapi:
    description: "Run the REST APIS"
    environment:
      CORE_TEST_REPORTS: /tmp/test-results
    machine: # executor type
      image: ubuntu-2004:202010-01
    parameters:
      airflow_version:
        description: "The Airflow version, for example '1.10.5'"
        type: string
      dev_build:
        description: "Indicate if this is a dev build"
        type: boolean
      extra_tags:
        description: "Extra args to pass to pass to Docker build command"
        type: string
      executor_type:
        description: "The type of executor to run on"
        default: "celery"
        type: string
      test_type:
        description: "The type of framework to run"
        default: ""
        type: string
    steps:
      - run: sudo apt-get -y update
      - run: sudo apt-get -y install git curl wget libpq-dev nodejs
      - run: curl -sL https://deb.nodesource.com/setup_12.x | sudo -E bash -
      - run: npm -v
      - run: node -v
      - run:
          name: Avoid hosts unknown for github
          command:   |
            [ ! -d ~/.ssh ] && mkdir ~/.ssh/
            echo -e "Host github.com\n\tStrictHostKeyChecking no\n" > ~/.ssh/config
      - run: git clone git@github.com:astronomer/qa-regression.git
      - run: git clone git@github.com:astronomer/qa-scenario-dags.git
      - run:
          name: Run Shell Script for creating deployments
          command:  |
            cd qa-regression
            ls
            chmod 744 *.sh
            echo "Run Deployment Script"
            bash ./create_deployment.sh << parameters.airflow_version >>  << parameters.executor_type >>  << parameters.test_type >>
            cd ..
      - run: mkdir -p $CORE_TEST_REPORTS/restapi/
      - run:
          name: Run the RESTAPI
          command:   |
            cd /home/circleci/project/qa-regression/postman_rest
            chmod 744 run_restapi_smoke.sh
            bash ./run_restapi_smoke.sh
            #cp result.out /tmp/test-results/restapi/
            #FOUND_404=0
            #FOUND_500=0
            #ASSERTION_ERRORS=0
            #grep -q "404 Not Found" result.out && FOUND_404=1
            #grep -q "500 INTERNAL SERVER ERROR" result.out && FOUND_500=1
            #grep -q "AssertionError" result.out && ASSERTION_ERRORS=1
            #if [[ "$FOUND_404" -eq 1 ]]; then echo "404 Errors Found";exit 1; fi
            #if [[ "$FOUND_500" -eq 1 ]]; then echo "500 Errors Found";exit 1; fi
            #if [[ "$ASSERTION_ERRORS" -eq 1 ]]; then echo "Assertion Errors Found";exit 1; fi
            #cd /home/circleci/project/qa-regression
            #chmod 744 delete_deployment.sh
            #bash ./delete_deployment.sh
      - store_artifacts:
          path: /tmp/test-results
      - store_test_results:
          path: /tmp/test-results
  qa-k8s:
    description: "Run the k8s DAGS"
    environment:
      CORE_TEST_REPORTS: /tmp/test-results
    machine: # executor type
      image: ubuntu-2004:202010-01
    parameters:
      airflow_version:
        description: "The Airflow version, for example '1.10.5'"
        type: string
      dev_build:
        description: "Indicate if this is a dev build"
        type: boolean
      extra_tags:
        description: "Extra args to pass to pass to Docker build command"
        type: string
      executor_type:
        description: "The type of executor to run on"
        default: "celery"
        type: string
      test_type:
        description: "The type of framework to run"
        default: ""
        type: string
    steps:
      - attach_workspace:
          at: ~/
      # - run: sudo apt-get -y update
      # - run: sudo apt-get -y install git curl wget libpq-dev nodejs
      # - run: curl -sL https://deb.nodesource.com/setup_12.x | sudo -E bash -
      # - run: npm -v
      # - run: node -v
      # - run:
      #     name: Avoid hosts unknown for github
      #     command:   |
      #       [ ! -d ~/.ssh ] && mkdir ~/.ssh/
      #       echo -e "Host github.com\n\tStrictHostKeyChecking no\n" > ~/.ssh/config
      # - run: git clone git@github.com:astronomer/qa-regression.git
      # - run: git clone git@github.com:astronomer/qa-scenario-dags.git
      # - run:
      #     name: Run Shell Script for creating deployments
      #     command:  |
      #       cd qa-regression
      #       ls
      #       chmod 744 *.sh
      #       echo "Run Deployment Script"
      #       bash ./create_deployment.sh << parameters.airflow_version >>  << parameters.executor_type >>  << parameters.test_type >>
      #       cd ..
      - run: mkdir -p $CORE_TEST_REPORTS/k8s/
      - run:
          name: Run the Pre-Reqs for running K8s
          command:   |
            cd /home/circleci/project/qa-regression
            chmod 744 pre-req-kubernetes.sh
            bash ./pre-req-kubernetes.sh
      - run:
          name: Run the DAGS
          command:   |
            cd /home/circleci/project/qa-scenario-dags/scripts/dagstest
            pip3 install -e .
            python3 -m venv env
            . env/bin/activate
            fileurl="/home/circle/project/env_current_api_url"
            export AIRFLOW_API_URL=$(cat "$fileurl")
            export DAGSTEST_DB_HOST=suleiman.db.elephantsql.com
            export DAGSTEST_DB_PORT=5432
            export DAGSTEST_DB_NAME=axsrggsp
            export DAGSTEST_DB_USER=axsrggsp
            export DAGSTEST_DB_PASSWORD=$QA_DAGSTEST_DB_PASSWORD
            #dags connections --headers "Authorization":"Bearer $QA_CLUSTER_SERVICEACCOUNT" -f /home/circleci/project/qa-scenario-dags/airflow_settings.yaml
            EXEC_TIME=$(date +%FT%TZ)
            dags trigger --headers "Authorization":"$QA_CLUSTER_SERVICEACCOUNT" -e $EXEC_TIME -t k8s
            sleep 420
            echo "Sleeping for 420s"
            dags results -e $EXEC_TIME --headers "Authorization":"Bearer $QA_CLUSTER_SERVICEACCOUNT" > $CORE_TEST_REPORTS/k8s/results_k8s_dags.out
            # FOUND_FAILURES=0
            # grep -q "failed" /tmp/test-results/results_k8s_dags.out && FOUND_FAILURES=1
            # cat /tmp/test-results/results_k8s_dags.out
            # if [[ "$FOUND_FAILURES" -eq 1 ]]; then echo "DAG Failures Found1";exit 1; fi
            # cd /home/circleci/project/qa-regression
            # chmod 744 delete_deployment.sh
            # bash ./delete_deployment.sh
      - store_artifacts:
          path: /tmp/test-results
      - store_test_results:
          path: /tmp/test-results
      - persist_to_workspace:
          root: .
          paths:
            - project
  qa-negative:
    description: "Run the Negative DAGS"
    environment:
      CORE_TEST_REPORTS: /tmp/test-results
    machine: # executor type
      image: ubuntu-2004:202010-01
    parameters:
      airflow_version:
        description: "The Airflow version, for example '1.10.5'"
        type: string
      dev_build:
        description: "Indicate if this is a dev build"
        type: boolean
      extra_tags:
        description: "Extra args to pass to pass to Docker build command"
        type: string
      executor_type:
        description: "The type of executor to run on"
        default: "celery"
        type: string
      test_type:
        description: "The type of framework to run"
        default: ""
        type: string
    steps:
      - attach_workspace:
          at: ~/
      # - run: sudo apt-get -y update
      # - run: sudo apt-get -y install git curl wget libpq-dev nodejs
      # - run: curl -sL https://deb.nodesource.com/setup_12.x | sudo -E bash -
      # - run: npm -v
      # - run: node -v
      # - run:
      #     name: Avoid hosts unknown for github
      #     command:   |
      #       [ ! -d ~/.ssh ] && mkdir ~/.ssh/
      #       echo -e "Host github.com\n\tStrictHostKeyChecking no\n" > ~/.ssh/config
      # - run: git clone git@github.com:astronomer/qa-regression.git
      # - run: git clone git@github.com:astronomer/qa-scenario-dags.git
      # - run:
      #     name: Run Shell Script for creating deployments
      #     command:  |
      #       cd qa-regression
      #       ls
      #       chmod 744 *.sh
      #       echo "Run Deployment Script"
      #       bash ./create_deployment.sh << parameters.airflow_version >>  << parameters.executor_type >>  << parameters.test_type >>
      #       cd ..
      - run: mkdir -p $CORE_TEST_REPORTS/negative/
      - run:
          name: Run the DAGS
          command:   |
            cd /home/circleci/project/qa-scenario-dags/scripts/dagstest
            pip3 install -e .
            python3 -m venv env
            . env/bin/activate
            fileurl="/home/circle/project/env_current_api_url"
            export AIRFLOW_API_URL=$(cat "$fileurl")
            export DAGSTEST_DB_HOST=suleiman.db.elephantsql.com
            export DAGSTEST_DB_PORT=5432
            export DAGSTEST_DB_NAME=axsrggsp
            export DAGSTEST_DB_USER=axsrggsp
            export DAGSTEST_DB_PASSWORD=$QA_DAGSTEST_DB_PASSWORD
            #dags connections --headers "Authorization":"Bearer $QA_CLUSTER_SERVICEACCOUNT" -f /home/circleci/project/qa-scenario-dags/airflow_settings.yaml
            EXEC_TIME=$(date +%FT%TZ)
            dags trigger --headers "Authorization":"$QA_CLUSTER_SERVICEACCOUNT" -e $EXEC_TIME -t negative
            sleep 500
            echo "Sleeping for 240s"
            dags results -e $EXEC_TIME --headers "Authorization":"Bearer $QA_CLUSTER_SERVICEACCOUNT" > $CORE_TEST_REPORTS/k8s/results_k8s_dags.out
            # FOUND_FAILURES=0
            # grep -q "failed" /tmp/test-results/results_k8s_dags.out && FOUND_FAILURES=1
            # cat /tmp/test-results/results_k8s_dags.out
            # if [[ "$FOUND_FAILURES" -eq 1 ]]; then echo "DAG Failures Found1";exit 1; fi
            # cd /home/circleci/project/qa-regression
            # chmod 744 delete_deployment.sh
            # bash ./delete_deployment.sh
      - store_artifacts:
          path: /tmp/test-results
      - store_test_results:
          path: /tmp/test-results
      - persist_to_workspace:
          root: .
          paths:
            - project
  qa-params:
    description: "Run the DAG Parameters"
    environment:
      CORE_TEST_REPORTS: /tmp/test-results
    machine: # executor type
      image: ubuntu-2004:202010-01
    parameters:
      airflow_version:
        description: "The Airflow version, for example '1.10.5'"
        type: string
      dev_build:
        description: "Indicate if this is a dev build"
        type: boolean
      extra_tags:
        description: "Extra args to pass to pass to Docker build command"
        type: string
      executor_type:
        description: "The type of executor to run on"
        default: "celery"
        type: string
      test_type:
        description: "The type of framework to run"
        default: ""
        type: string
    steps:
      - attach_workspace:
          at: ~/
      # - run: sudo apt-get -y update
      # - run: sudo apt-get -y install git curl wget libpq-dev nodejs
      # - run: curl -sL https://deb.nodesource.com/setup_12.x | sudo -E bash -
      # - run: npm -v
      # - run: node -v
      # - run:
      #     name: Avoid hosts unknown for github
      #     command:   |
      #       [ ! -d ~/.ssh ] && mkdir ~/.ssh/
      #       echo -e "Host github.com\n\tStrictHostKeyChecking no\n" > ~/.ssh/config
      # - run: git clone git@github.com:astronomer/qa-regression.git
      # - run: git clone git@github.com:astronomer/qa-scenario-dags.git
      # - run:
      #     name: Run Shell Script for creating deployments
      #     command:  |
      #       cd qa-regression
      #       ls
      #       chmod 744 *.sh
      #       echo "Run Deployment Script"
      #       bash ./create_deployment.sh << parameters.airflow_version >>  << parameters.executor_type >>  << parameters.test_type >>
      #       cd ..
      - run: mkdir -p $CORE_TEST_REPORTS/dag-params
      - run:
          name: Run the DAGS
          command:   |
            cd /home/circleci/project/qa-scenario-dags/scripts/dagstest
            pip3 install -e .
            python3 -m venv env
            . env/bin/activate
            fileurl="/home/circle/project/env_current_api_url"
            export AIRFLOW_API_URL=$(cat "$fileurl")
            export DAGSTEST_DB_HOST=suleiman.db.elephantsql.com
            export DAGSTEST_DB_PORT=5432
            export DAGSTEST_DB_NAME=axsrggsp
            export DAGSTEST_DB_USER=axsrggsp
            export DAGSTEST_DB_PASSWORD=$QA_DAGSTEST_DB_PASSWORD
            #dags connections --headers "Authorization":"Bearer $QA_CLUSTER_SERVICEACCOUNT" -f /home/circleci/project/qa-scenario-dags/airflow_settings.yaml
            EXEC_TIME=$(date +%FT%TZ)
            dags trigger --headers "Authorization":"$QA_CLUSTER_SERVICEACCOUNT" -e $EXEC_TIME -t dagparams
            sleep 500
            echo "Sleeping for 240s"
            dags results -e $EXEC_TIME --headers "Authorization":"Bearer $QA_CLUSTER_SERVICEACCOUNT" > $CORE_TEST_REPORTS/dag-params/results_dag-params_dags.out
            # FOUND_FAILURES=0
            # grep -q "failed" /tmp/test-results/results_dag-params_dags.out && FOUND_FAILURES=1
            # cat /tmp/test-results/results_dag-params_dags.out
            # if [[ "$FOUND_FAILURES" -eq 1 ]]; then echo "DAG Failures Found1";exit 1; fi
            # cd /home/circleci/project/qa-regression
            # chmod 744 delete_deployment.sh
            # bash ./delete_deployment.sh
      - store_artifacts:
          path: /tmp/test-results
      - store_test_results:
          path: /tmp/test-results
      - persist_to_workspace:
          root: .
          paths:
            - project
  static-checks:
    executor: machine-executor
    description: Static Checks
    steps:
      - checkout
      - run:
          name: Load archived Docker image
          command: |
            pyenv global 3.8.5
            pip install -r .circleci/test-requirements.txt
            pre-commit run --all-files
  build:
    executor: docker-executor
    description: Build Airflow images
    parameters:
      airflow_version:
        description: "The Airflow version, for example '1.10.5'"
        type: string
      distribution_name:
        description: "The base distribution of the container"
        type: string
      dev_build:
        description: "Indicate if this is a dev build"
        type: boolean
      extra_args:
        description: "Extra args to pass to pass to Docker build command"
        default: ""
        type: string
    steps:
      - docker-build-base-and-onbuild:
          airflow_version: "<< parameters.airflow_version >>"
          distribution_name: "<< parameters.distribution_name >>"
          extra_args: "<< parameters.extra_args >>"
  test:
    executor: machine-executor
    description: Test Airflow images
    parameters:
      tag:
        type: string
    steps:
      - airflow-image-test:
          tag: "<< parameters.tag >>"
  scan-clair:
    executor: clair-scanner/default
    description: Test Airflow images
    parameters:
      airflow_version:
        description: "The Airflow version, for example '1.10.5'"
        type: string
      distribution_name:
        description: "The base distribution of the container"
        type: string
    steps:
      - clair-scan:
          image_name: "ap-airflow:<< parameters.airflow_version >>-<< parameters.distribution_name >>"
  scan-trivy:
    docker:
      - image: docker:18.09-git
    description: "Trivy: Vulnerability scanner a Docker image"
    parameters:
      airflow_version:
        description: "The Airflow version, for example '1.10.5'"
        type: string
      distribution_name:
        description: "The base distribution of the container"
        type: string
      distribution:
        description: "The distribution without onbuild"
        type: string
    steps:
      - checkout
      - setup_remote_docker
      - attach_workspace:
          at: /tmp/workspace
      - run:
          name: Load archived Docker image
          command: docker load -i /tmp/workspace/saved-images/ap-airflow:<< parameters.airflow_version >>-<< parameters.distribution_name >>.tar
      - run:
          name: Install trivy
          command: |
            apk add --update-cache --upgrade curl rpm
            curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/master/contrib/install.sh | sh -s -- -b /usr/local/bin
      - restore_cache:
          keys:
            - trivy-cache
      - run:
          name: Scan the local image with trivy
          command: |
            trivy \
              --ignorefile "./<< parameters.airflow_version >>/<< parameters.distribution >>/trivyignore" \
              --cache-dir /tmp/workspace/trivy-cache \
              --ignore-unfixed -s HIGH,CRITICAL \
              --exit-code 1 \
              --no-progress "ap-airflow:<< parameters.airflow_version >>-<< parameters.distribution_name >>"
      - save_cache:
          key: trivy-cache
          paths:
            - /tmp/workspace/trivy-cache
  push:
    executor: docker-executor
    description: Push Airflow images
    parameters:
      dev_build:
        description: "Indicate if this is a dev build"
        type: boolean
      tag:
        type: string
      extra_tags:
        type: string
        default: ""
      prod_docker_repo_docker_hub:
        description: "The docker repo to tag and push to, for example 'quay.io/astronomer/ap-airflow'"
        default: "astronomerinc/ap-airflow"
        type: string
      dev_docker_repo_docker_hub:
        description: "The docker repo to tag and push to, for example 'quay.io/astronomer/ap-airflow'"
        default: "astronomerio/ap-airflow"
        type: string
      prod_docker_repo_quay_io:
        description: "The docker repo to tag and push to, for example 'quay.io/astronomer/ap-airflow'"
        default: "quay.io/astronomer/ap-airflow"
        type: string
      dev_docker_repo_quay_io:
        description: "The docker repo to tag and push to, for example 'quay.io/astronomer/ap-airflow'"
        default: "quay.io/astronomer/ap-airflow-dev"
        type: string
      prod_core_docker_repo_quay_io:
        description: "The docker repo to tag and push to, for example 'quay.io/astronomer/ap-airflow'"
        default: quay.io/astronomer/core
        type: string
    steps:
      - push:
          dev_release: "<< parameters.dev_build >>"
          extra_tags: "<< parameters.extra_tags >>"
          tag: "<< parameters.tag >>"
          prod_docker_repo_docker_hub: "<< parameters.prod_docker_repo_docker_hub >>"
          dev_docker_repo_docker_hub: "<< parameters.dev_docker_repo_docker_hub >>"
          prod_docker_repo_quay_io: "<< parameters.prod_docker_repo_quay_io >>"
          dev_docker_repo_quay_io: "<< parameters.dev_docker_repo_quay_io >>"
          prod_core_docker_repo_quay_io: "<< parameters.prod_core_docker_repo_quay_io >>"
orbs:
  clair-scanner: ovotech/clair-scanner@1.6.0
executors:
  docker-executor:
    docker:
      - image: circleci/python:3
  machine-executor:
    machine:
      image: ubuntu-2004:202008-01

commands:
  docker-build-base-and-onbuild:
    description: "Build Airflow images to use with the Astronomer platform"
    parameters:
      airflow_version:
        type: string
        default: 1.10.5
      distribution_name:
        type: string
        default: alpine
      extra_args:
        description: "Extra args to pass to pass to Docker build command"
        default: ""
        type: string
    steps:
      - checkout
      - setup_remote_docker:
          docker_layer_caching: true
      - docker-build:
          image_name: "ap-airflow:<< parameters.airflow_version >>-<< parameters.distribution_name >>"
          path: "<< parameters.airflow_version >>/<< parameters.distribution_name >>"
          extra_args: "<< parameters.extra_args >>"
      - docker-build:
          image_name: "ap-airflow:<< parameters.airflow_version >>-<< parameters.distribution_name >>-onbuild"
          path: "common/"
          dockerfile: "Dockerfile.onbuild-<< parameters.distribution_name >>"
          extra_args: "--build-arg baseimage=ap-airflow:<< parameters.airflow_version >>-<< parameters.distribution_name >>"
      - persist_to_workspace:
          root: .
          paths:
            - saved-images/
  docker-build:
    description: "Build a Docker image"
    parameters:
      dockerfile:
        type: string
        default: Dockerfile
      path:
        type: string
        default: "."
      image_name:
        type: string
        default: $CIRCLE_PROJECT_REPONAME
      extra_args:
        type: string
        default: ""
    steps:
      - run:
          name: Build the Docker image
          command: |
            set -xe
            mkdir -p saved-images/"$(dirname '<< parameters.image_name >>')"
            docker build \
              --tag '<< parameters.image_name >>' \
              --label io.astronomer.docker.build_time="$(date +%s)" \
              --label io.astronomer.repo.commit_sha="${CIRCLE_SHA1}" \
              --label io.astronomer.repo.url="${CIRCLE_REPOSITORY_URL}" \
              --label io.astronomer.ci.build_url="${CIRCLE_BUILD_URL}" \
              --file '<< parameters.path>>/<< parameters.dockerfile >>' \
              --build-arg "BUILD_NUMBER=${CIRCLE_BUILD_NUM}" \
              --build-arg "REPO_BRANCH=${CIRCLE_BRANCH}" \
              << parameters.extra_args >> '<< parameters.path >>'
            docker save -o saved-images/<< parameters.image_name >>.tar '<< parameters.image_name >>'
  airflow-image-test:
    description: Test an Airflow image
    parameters:
      tag:
        type: string
    steps:
      - checkout
      - attach_workspace:
          at: /tmp/workspace
      - run:
          name: Load archived Airflow Docker image
          command: |
            docker load -i /tmp/workspace/saved-images/ap-airflow:<< parameters.tag >>.tar
            docker load -i /tmp/workspace/saved-images/ap-airflow:<< parameters.tag >>-onbuild.tar
      - run:
          name: Test Airflow Docker images (Base + Onbuild)
          command: |
            set -e
            pyenv global 3.8.5
            pip install -r .circleci/test-requirements.txt
            .circleci/bin/test-airflow 'ap-airflow' '<< parameters.tag >>'
      - store_test_results:
          path: /tmp/test-reports
  push:
    description: "Push a Docker image to DockerHub"
    parameters:
      dev_release:
        description: "Indicate if this is a dev release"
        default: true
        type: boolean
      extra_tags:
        type: string
        default: ""
      tag:
        type: string
      image_name:
        default: ap-airflow
        type: string
      prod_docker_repo_docker_hub:
        default: astronomerinc/ap-airflow
        type: string
      dev_docker_repo_docker_hub:
        default: "astronomerio/ap-airflow"
        type: string
      prod_docker_repo_quay_io:
        default: quay.io/astronomer/ap-airflow
        type: string
      dev_docker_repo_quay_io:
        default: "quay.io/astronomer/ap-airflow-dev"
        type: string
      prod_core_docker_repo_quay_io:
        default: quay.io/astronomer/core
        type: string
    steps:
      - attach_workspace:
          at: /tmp/workspace
      - setup_remote_docker
      - run:
          name: Load archived Docker image
          command: docker load -i '/tmp/workspace/saved-images/ap-airflow:<< parameters.tag >>.tar'
      - run:
          name: Login to DockerHub
          command: echo "$DOCKER_PASSWORD" | docker login --username "$DOCKER_USERNAME" --password-stdin docker.io
      - run:
          name: Login to Quay.io
          command: echo "$QUAY_PASSWORD" | docker login --username "$QUAY_USERNAME" --password-stdin quay.io
      - run:
          name: Push Docker image(s)
          command: |
            set -e
            image="ap-airflow:<< parameters.tag >>"
            set -x
            export NEW_POINT_RELEASE=true
            for tag in $(echo "<< parameters.extra_tags >>" | sed "s/,/ /g");
            do
              if DOCKER_CLI_EXPERIMENTAL=enabled docker manifest inspect "<< parameters.prod_docker_repo_docker_hub >>:$tag" >/dev/null 2>&1; then
                echo "Image with Tag ("<< parameters.prod_docker_repo_docker_hub >>:$tag") already exists"
                export NEW_POINT_RELEASE=false
              elif [[ "$tag" == "<< parameters.tag >>-$CIRCLE_BUILD_NUM" ]] || <<parameters.dev_release >> ; then
                echo "Tagging and pushing image <<parameters.image_name>>:$tag to << parameters.dev_docker_repo_quay_io >>"
                docker tag "$image" "<< parameters.dev_docker_repo_quay_io >>:${tag}"
                docker push "<< parameters.dev_docker_repo_quay_io >>:${tag}"

                echo "Tagging and pushing image <<parameters.image_name>>:${tag} to << parameters.dev_docker_repo_docker_hub >>"
                docker tag "$image" "<< parameters.dev_docker_repo_docker_hub >>:$tag"
                docker push "<< parameters.dev_docker_repo_docker_hub >>:${tag}"

                echo "Build TAG artifact"

                if [[ "${tag}" == *"onbuild-"* ]]; then
                  [ ! -d /tmp/build ] && mkdir /tmp/build
                  airflow_release=$(echo $tag | cut -d "-" -f 1)
                  echo ${tag} > /tmp/build/current_tag_$airflow_release.release
                fi

              else
                echo "Tagging and pushing image <<parameters.image_name>>:$tag to << parameters.prod_docker_repo_quay_io >>"
                docker tag "$image" "<< parameters.prod_docker_repo_quay_io >>:${tag}"
                docker push "<< parameters.prod_docker_repo_quay_io >>:${tag}"

                echo "Tagging and pushing image <<parameters.image_name>>:$tag to << parameters.prod_docker_repo_docker_hub >>"
                docker tag "$image" "<< parameters.prod_docker_repo_docker_hub >>:${tag}"
                docker push "<< parameters.prod_docker_repo_docker_hub >>:${tag}"

                echo "Tagging and pushing image <<parameters.image_name>>:$tag to << parameters.prod_core_docker_repo_quay_io >>"
                docker tag "$image" "<< parameters.prod_core_docker_repo_quay_io >>:${tag}"
                docker push "<< parameters.prod_core_docker_repo_quay_io >>:${tag}"

                echo "Tagging and pushing image <<parameters.image_name>>:$tag to << parameters.dev_docker_repo_quay_io >>"
                docker tag "$image" "<< parameters.dev_docker_repo_quay_io >>:${tag}"
                docker push "<< parameters.dev_docker_repo_quay_io >>:${tag}"

                echo "Tagging and pushing image <<parameters.image_name>>:$tag to << parameters.dev_docker_repo_docker_hub >>"
                docker tag "$image" "<< parameters.dev_docker_repo_docker_hub >>:${tag}"
                docker push "<< parameters.dev_docker_repo_docker_hub >>:${tag}"
              fi
            done

            # The following code block publishes Moving Master builds (e.g. 1.10.13-buster-onbuild)
            # as compared to above code-blocks that publish images with build-number & immutable tag
            # e.g (e.g. 1.10.13-1-buster-onbuild and 1.10.13-buster-onbuild-24119)
            if $NEW_POINT_RELEASE ; then
              if ! <<parameters.dev_release >> ; then
                # If it is not a Dev Release publish the image to Prod Repos
                echo "Tagging and pushing image <<parameters.image_name>>:<< parameters.tag >> to << parameters.prod_docker_repo_quay_io >>"
                docker tag "$image" "<< parameters.prod_docker_repo_quay_io >>:<< parameters.tag >>"
                docker push "<< parameters.prod_docker_repo_quay_io >>:<< parameters.tag >>"

                echo "Tagging and pushing image <<parameters.image_name>>:<< parameters.tag >> to << parameters.prod_docker_repo_docker_hub >>"
                docker tag "$image" "<< parameters.prod_docker_repo_docker_hub >>:<< parameters.tag >>"
                docker push "<< parameters.prod_docker_repo_docker_hub >>:<< parameters.tag >>"

                echo "Tagging and pushing image <<parameters.image_name>>:<< parameters.tag >> to << parameters.prod_core_docker_repo_quay_io >>"
                docker tag "$image" "<< parameters.prod_core_docker_repo_quay_io >>:<< parameters.tag >>"
                docker push "<< parameters.prod_core_docker_repo_quay_io >>:<< parameters.tag >>"
              fi

              echo "Tagging and pushing image <<parameters.image_name>>:<< parameters.tag >> to << parameters.dev_docker_repo_docker_hub >>"
              docker tag "$image" "<< parameters.dev_docker_repo_docker_hub >>:<< parameters.tag >>"
              docker push "<< parameters.dev_docker_repo_docker_hub >>:<< parameters.tag >>"

              echo "Tagging and pushing image <<parameters.image_name>>:<< parameters.tag >> to << parameters.dev_docker_repo_quay_io >>"
              docker tag "$image" "<< parameters.dev_docker_repo_quay_io >>:<< parameters.tag >>"
              docker push "<< parameters.dev_docker_repo_quay_io >>:<< parameters.tag >>"
            else
              echo "Image with Tag ($image) not pushed as it is not a new point release"
            fi
            set +x
      - store_artifacts:
          path: /tmp/build/current_tag
  clair-scan:
    description: "Vulnerability scan a Docker image"
    parameters:
      image_name:
        type: string
        default: $CIRCLE_PROJECT_REPONAME
      allowlist:
        type: string
        default: cve-allowlist.yaml
    steps:
      - checkout
      - setup_remote_docker
      - attach_workspace:
          at: /tmp/workspace
      - run:
          name: Load archived Docker image
          command: docker load -i /tmp/workspace/saved-images/<< parameters.image_name >>.tar
      - modified-orb:
          allowlist: "<< parameters.allowlist >>"
          image: "<< parameters.image_name >>"
  wait_workflow_orb:
    description: |
        This command waits for running workflows of a kind on a given branch to complete. Adding This
        as the first step in a job will make the job wait till it is the only running workflow.
        This requires a Circle CI token be set as $CIRCLE_API_USER_TOKEN
    parameters:
        branch-to-consider:
            default: master
            description: The branch on which we will wait for workflows. If set to "all" this will wait across all branches
            type: string
        kill-gracefully:
            default: "true"
            description: If true and time exceeds max wait time, dies without failing the job
            type: string
        max-wait-time:
            default: "1800"
            description: |
                The max wait time in seconds a job should wait for before killing itself.
            type: string
        run-on-branch:
            default: '*'
            description: |
                The branches to actually wait on. By default this waits on all branches. If set to anything but
                '*' the wait will run only on the specified branch
            type: string
        sleep-time-between-checks:
            default: "30"
            description: How long to sleep between checks.
            type: string
        vcs-type:
            default: github
            description: What is the VCS for this project
            type: string
        workflow-name:
            default: '*'
            description: The type of workflows to wait for. This can be a regex
            type: string
        airflow-version:
            default: '2.0.0'
            description: Airflow version to be followed
            type: string
    steps:
        - run:
            command: |
                if [ -z "$BASH" ]; then
                  echo Bash not installed.
                  exit 1
                fi
                hash jq 2>/dev/null || { echo >&2 "jq is not installed.  Aborting."; exit 1; }
                if [[ "$CIRCLE_API_USER_TOKEN" == "" ]]; then
                  echo "CIRCLE_API_USER_TOKEN not set. Set a token to access the circle API in the env var CIRCLE_API_USER_TOKEN";
                  exit 1;
                fi

                if [[ "<< parameters.run-on-branch >>" != "*" && "<< parameters.run-on-branch >>" != "$CIRCLE_BRANCH" ]]; then
                  echo "Chosen to run only on << parameters.run-on-branch >> and currently we are on $CIRCLE_BRANCH, exiting";
                  exit 0;
                fi

                slug="<< parameters.vcs-type >>/${CIRCLE_PROJECT_USERNAME}/${CIRCLE_PROJECT_REPONAME}"
                branch_to_consider="<< parameters.branch-to-consider >>"

                # Assume there is one workflow running.
                total_workflows_running="1"
                can_i_run="true"

                mkdir -p /tmp/swissknife

                # This is a global variable used to get return value for get_workflow_start_time
                workflow_start_time=""

                get_workflow_start_time() {
                  wf_url="https://circleci.com/api/v2/workflow/%7B$1%7D?circle-token=${CIRCLE_API_USER_TOKEN}"
                  echo $wf_url
                  curl -f -s $wf_url > /tmp/swissknife/wf_$1.json
                  workflow_start_time=$(jq '.created_at' /tmp/swissknife/wf_$1.json)
                  echo "Reached"
                }

                echo "Er:"$workflow_start_time

                get_num_running_workflows() {
                  running_job_prefix="https://circleci.com/api/v1.1/project/$slug";
                  running_jobs_suffix="?circle-token=${CIRCLE_API_USER_TOKEN}&filter=running&limit=100";
                  q_jobs_suffix="?circle-token=${CIRCLE_API_USER_TOKEN}&filter=queued&limit=100";
                  failed_jobs_suffix="?circle-token=${CIRCLE_API_USER_TOKEN}&limit=1";
                  running_jobs_branch="";
                  if [[ "$branch_to_consider" != "all" ]]; then
                    running_jobs_branch="/tree/$branch_to_consider";
                  fi
                  running_jobs_url="${running_job_prefix}${running_jobs_branch}${running_jobs_suffix}"
                  q_jobs_url="${running_job_prefix}${running_jobs_branch}${q_jobs_suffix}"
                  echo "ER:"$running_jobs_url
                  curl -f -s $running_jobs_url > /tmp/swissknife/running_jobs.json
                  curl -f -s $q_jobs_url > /tmp/swissknife/q_jobs.json
                  total_workflows_running=$(jq --arg curworkflow "$CIRCLE_WORKFLOW_ID" '[unique_by(.workflows.workflow_id) | .[] | select(.workflows.workflow_name|test("<< parameters.workflow-name >>")) | select(.workflows.workflow_id|test($curworkflow)|not) ] | length' /tmp/swissknife/running_jobs.json)
                  total_workflows_q=$(jq --arg curworkflow "$CIRCLE_WORKFLOW_ID" '[unique_by(.workflows.workflow_id) | .[] | select(.workflows.workflow_name|test("<< parameters.workflow-name >>")) | select(.workflows.workflow_id|test($curworkflow)|not) ] | length' /tmp/swissknife/q_jobs.json)
                  # If no other workflows are running, we're good. just return
                  if [ "$total_workflows_running" == "0" ] && [ "$total_workflows_q" == "0" ]; then
                    #Validate if the workflow failed
                    failed_jobs_url="${running_job_prefix}${running_jobs_branch}${failed_jobs_suffix}"
                    curl -f -s $failed_jobs_url > /tmp/swissknife/failed_jobs.json
                    total_workflows_failed=$(jq 'unique_by(.workflows.workflow_id) | .[] | select(.workflows.workflow_name|test("certified-airflow")) .status' /tmp/swissknife/failed_jobs.json)
                    echo "RecentJobStatus:"$total_workflows_failed
                    if [ "$total_workflows_failed" == "\"failed\"" ] || [ "$total_workflows_failed" == "\"cancelled\"" ]; then
                      kill_workflow
                    else
                      return 0
                    fi
                  fi
                  # Finding all running workflows
                  jq -r --arg curworkflow "$CIRCLE_WORKFLOW_ID" 'unique_by(.workflows.workflow_id) | .[] | select(.workflows.workflow_name|test("<< parameters.workflow-name >>")) | select(.workflows.workflow_id|test($curworkflow)|not) | .workflows.workflow_id' /tmp/swissknife/running_jobs.json > /tmp/swissknife/running_workflows.txt
                  cat /tmp/swissknife/running_workflows.txt
                  echo "ER:Completed -JQ2"
                  # get_workflow_start_time $CIRCLE_WORKFLOW_ID
                  # current_workflow_start_time=$workflow_start_time
                  can_i_run="false"
                  # while IFS= read -r line
                  # do
                  #   echo "Checking info for workflow:$line"
                  #   get_workflow_start_time $line
                  #   running_wf_start_time=$workflow_start_time
                  #   if [[ $running_wf_start_time < $current_workflow_start_time ]] ; then
                  #     can_i_run="false"
                  #     break
                  #   fi
                  # done < /tmp/swissknife/running_workflows.txt

                  echo "ER:Completed"
                }

                kill_workflow(){
                  echo "Cancelleing workflow by cancelling build ${CIRCLE_BUILD_NUM}"
                  cancel_workflow_url="https://circleci.com/api/v1.1/project/$slug/${CIRCLE_BUILD_NUM}/cancel?circle-token=${CIRCLE_API_USER_TOKEN}"
                  curl -s -X POST $cancel_workflow_url > /dev/null
                  # Give Circle CI enough time to kill this workflow
                  sleep 30
                  # If the job wasnt canceled in 30 seconds fail.
                  exit 1;
                }

                current_wait_time=0

                while true; do
                  get_num_running_workflows
                  echo "Enter Loop"
                  if [[ "$total_workflows_running" == "0" || "$can_i_run" == "true" ]]; then
                    echo "$total_workflows_running"
                    echo "$can_i_run"
                    echo "Its finally my turn. exiting"
                    exit 0
                  else
                    echo "Looks like $total_workflows_running are still running. and can_i_run:$can_i_run"
                    echo "Going to sleep for << parameters.sleep-time-between-checks >>"
                    sleep << parameters.sleep-time-between-checks >>
                    current_wait_time=$(( current_wait_time + << parameters.sleep-time-between-checks >> ))
                  fi

                  if (( $current_wait_time > << parameters.max-wait-time >> )); then
                    if [[ "<< parameters.kill-gracefully >>" == "true" ]]; then
                      echo "Killing workflow by cancelling";
                      kill_workflow
                    else
                      echo "Killing workflow by exiting forcefully";
                      exit 1;
                    fi
                  fi
                done
            name: Swissknife - Wait for workflows - ER
  # TODO: move to an Astronomer orbs repo, publish orb.
  # This is to work around an issue in the provided orb https://github.com/ovotech/circleci-orbs/issues/89.
  modified-orb:
    description: "Scan an image for vulnerabilities"
    parameters:
      image:
        type: "string"
        description: "Name of the image to scan"
        default: ""
      image_file:
        type: "string"
        description: "Path to a file of images to scan"
        default: ""
      allowlist:
        type: "string"
        description: "Path to a CVE allowlist"
        default: ""
      severity_threshold:
        type: "string"
        description: "The threshold (equal and above) at which discovered vulnerabilities are reported. May be 'Defcon1', 'Critical', 'High', 'Medium', 'Low', 'Negligible' or 'Unknown'"
        default: "High"
      fail_on_discovered_vulnerabilities:
        type: "boolean"
        description: "Fail command when vulnerabilities at severity equal to or above the threshold are discovered"
        default: true
      fail_on_unsupported_images:
        type: "boolean"
        description: "Fail command when image cannot be scanned for vulnerabilities"
        default: true
      disable_verbose_console_output:
        type: "boolean"
        description: "Disable verbose console output"
        default: false
      docker_tar_dir:
        type: "string"
        description: "Path of directory that Docker tarballs are stored"
        default: "/docker-tars"
    steps:
      - run:
          name: "Vulnerability scan"
          command: |
            #!/usr/bin/env bash

            set -xe

            DOCKER_TAR_DIR="<< parameters.docker_tar_dir >>"

            if [ -z "<< parameters.image_file >><< parameters.image >>" ] && [ -z "$(ls -A "$DOCKER_TAR_DIR" 2>/dev/null)" ]; then
                echo "image_file or image parameters or docker tarballs must be present"
                exit 255
            fi

            REPORT_DIR=/clair-reports
            mkdir $REPORT_DIR

            DB=$(docker run -p 5432:5432 -d arminc/clair-db:latest)
            CLAIR=$(docker run -p 6060:6060 --link "$DB":postgres -d arminc/clair-local-scan:latest)
            CLAIR_SCANNER=$(docker run -v /var/run/docker.sock:/var/run/docker.sock -d ovotech/clair-scanner@sha256:8a4f920b4e7e40dbcec4a6168263d45d3385f2970ee33e5135dd0e3b75d39c75 tail -f /dev/null)

            clair_ip=$(docker exec -it "$CLAIR" hostname -i | grep -oE '[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+')
            scanner_ip=$(docker exec -it "$CLAIR_SCANNER" hostname -i | grep -oE '[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+')

            if [ -n "<< parameters.allowlist >>" ]; then
                cat "<< parameters.allowlist >>"
                docker cp "<< parameters.allowlist >>" "$CLAIR_SCANNER:/allowlist.yml"

                allowlist="-w /allowlist.yml"
            fi

            function scan() {
                local image=$1
                # replace forward-slashes and colons with underscores
                munged_image=$(echo "$image" | sed 's/\//_/g' | sed 's/:/_/g')
                sanitised_image_filename="${munged_image}.json"
                local ret=0
                local docker_cmd=(docker exec -it "$CLAIR_SCANNER" clair-scanner \
                    --ip "$scanner_ip" \
                    --clair=http://"$clair_ip":6060 \
                    -t "<< parameters.severity_threshold >>" \
                    --report "/$sanitised_image_filename" \
                    --log "/log.json" \
                    --whitelist /allowlist.yml \
                    --reportAll=true \
                    --exit-when-no-features=false \
                    "$image")

                # if verbose output is disabled, analyse status code for more fine-grained output
                if [ "<< parameters.disable_verbose_console_output >>" == "true" ];then
                    "${docker_cmd[@]}" > /dev/null 2>&1 || ret=$?
                else
                    "${docker_cmd[@]}" 2>&1 || ret=$?
                fi
                if [ "$ret" -eq 0 ]; then
                    echo "No unapproved vulnerabilities"
                elif [ "$ret" -eq 1 ]; then
                    echo "Unapproved vulnerabilities found"
                    if [ "<< parameters.fail_on_discovered_vulnerabilities >>" == "true" ];then
                        EXIT_STATUS=1
                    fi
                elif [ "$ret" -eq 5 ]; then
                    echo "Image was not scanned, not supported."
                    if [ "<< parameters.fail_on_unsupported_images >>" == "true" ];then
                        EXIT_STATUS=1
                    fi
                else
                    echo "Unknown clair-scanner return code $ret."
                    EXIT_STATUS=1
                fi

                docker cp "$CLAIR_SCANNER:/$sanitised_image_filename" "$REPORT_DIR/$sanitised_image_filename" || true
            }

            EXIT_STATUS=0

            for entry in "$DOCKER_TAR_DIR"/*.tar; do
                [ -e "$entry" ] || continue
                images=$(docker load -i "$entry" | sed -e 's/Loaded image: //g')
                for image in $images; do
                    scan "$image"
                done
            done

            if [ -n "<< parameters.image_file >>" ]; then
                images=$(cat "<< parameters.image_file >>")
                for image in $images; do
                    scan "$image"
                done
            fi
            if [ -n "<< parameters.image >>" ]; then
                image="<< parameters.image >>"
                scan "$image"
            fi

            exit $EXIT_STATUS
      - store_artifacts:
          path: /clair-reports
